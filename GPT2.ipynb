{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyO0o8eGNLo3fl92KXN7scQX"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "RSXiOUrucBSi",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738430185691,
     "user_tz": 300,
     "elapsed": 19308,
     "user": {
      "displayName": "Jhenner Tigreros",
      "userId": "10282691994518148206"
     }
    },
    "outputId": "3d7f7e83-6a40-46a4-869c-a5ac8bcb5031",
    "ExecuteTime": {
     "end_time": "2025-09-17T05:49:10.365177Z",
     "start_time": "2025-09-17T05:48:19.072425Z"
    }
   },
   "source": [
    "!pip install einops xformers np"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting xformers\n",
      "  Downloading xformers-0.0.32.post2-cp39-abi3-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting np\n",
      "  Downloading np-1.0.2.tar.gz (7.4 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy in d:\\profile\\programing\\ia\\basicllmsmodels\\.venv\\lib\\site-packages (from xformers) (2.3.3)\n",
      "Requirement already satisfied: torch==2.8.0 in d:\\profile\\programing\\ia\\basicllmsmodels\\.venv\\lib\\site-packages (from xformers) (2.8.0)\n",
      "Requirement already satisfied: filelock in d:\\profile\\programing\\ia\\basicllmsmodels\\.venv\\lib\\site-packages (from torch==2.8.0->xformers) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\profile\\programing\\ia\\basicllmsmodels\\.venv\\lib\\site-packages (from torch==2.8.0->xformers) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\profile\\programing\\ia\\basicllmsmodels\\.venv\\lib\\site-packages (from torch==2.8.0->xformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\profile\\programing\\ia\\basicllmsmodels\\.venv\\lib\\site-packages (from torch==2.8.0->xformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\profile\\programing\\ia\\basicllmsmodels\\.venv\\lib\\site-packages (from torch==2.8.0->xformers) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\profile\\programing\\ia\\basicllmsmodels\\.venv\\lib\\site-packages (from torch==2.8.0->xformers) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in d:\\profile\\programing\\ia\\basicllmsmodels\\.venv\\lib\\site-packages (from torch==2.8.0->xformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\profile\\programing\\ia\\basicllmsmodels\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch==2.8.0->xformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\profile\\programing\\ia\\basicllmsmodels\\.venv\\lib\\site-packages (from jinja2->torch==2.8.0->xformers) (3.0.2)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading xformers-0.0.32.post2-cp39-abi3-win_amd64.whl (100.2 MB)\n",
      "   ---------------------------------------- 0.0/100.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/100.2 MB 6.7 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 2.6/100.2 MB 7.5 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 3.7/100.2 MB 6.6 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 5.2/100.2 MB 6.8 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 6.8/100.2 MB 6.9 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 8.4/100.2 MB 7.0 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 10.2/100.2 MB 7.2 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 11.8/100.2 MB 7.3 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 13.6/100.2 MB 7.4 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 15.5/100.2 MB 7.5 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 17.0/100.2 MB 7.5 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 18.6/100.2 MB 7.5 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 20.2/100.2 MB 7.4 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 21.5/100.2 MB 7.4 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 22.8/100.2 MB 7.2 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 23.9/100.2 MB 7.1 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 24.9/100.2 MB 7.0 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 26.0/100.2 MB 6.9 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 26.7/100.2 MB 6.8 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 28.3/100.2 MB 6.8 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 30.1/100.2 MB 6.8 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 31.7/100.2 MB 6.9 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 33.6/100.2 MB 7.0 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 35.4/100.2 MB 7.0 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 37.2/100.2 MB 7.1 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 38.8/100.2 MB 7.1 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 40.6/100.2 MB 7.1 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 42.5/100.2 MB 7.2 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 44.0/100.2 MB 7.2 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 45.9/100.2 MB 7.2 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 47.4/100.2 MB 7.2 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 48.0/100.2 MB 7.2 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 50.3/100.2 MB 7.2 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 51.9/100.2 MB 7.2 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 52.7/100.2 MB 7.1 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 53.2/100.2 MB 7.0 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 54.0/100.2 MB 6.9 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 54.5/100.2 MB 6.8 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 55.6/100.2 MB 6.7 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 56.4/100.2 MB 6.7 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 58.5/100.2 MB 6.7 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 60.8/100.2 MB 6.8 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 63.2/100.2 MB 6.9 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 64.7/100.2 MB 6.9 MB/s eta 0:00:06\n",
      "   -------------------------- ------------- 65.8/100.2 MB 6.9 MB/s eta 0:00:06\n",
      "   -------------------------- ------------- 66.6/100.2 MB 6.8 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 67.6/100.2 MB 6.8 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 68.4/100.2 MB 6.7 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 69.5/100.2 MB 6.7 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 69.7/100.2 MB 6.6 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 70.3/100.2 MB 6.5 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 70.8/100.2 MB 6.4 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 71.6/100.2 MB 6.4 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 72.1/100.2 MB 6.3 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 72.9/100.2 MB 6.2 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 73.7/100.2 MB 6.2 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 74.4/100.2 MB 6.1 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 75.2/100.2 MB 6.1 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 76.0/100.2 MB 6.1 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 76.5/100.2 MB 6.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 77.1/100.2 MB 6.0 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 77.9/100.2 MB 5.9 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 78.4/100.2 MB 5.9 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 79.2/100.2 MB 5.8 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 80.0/100.2 MB 5.8 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 80.5/100.2 MB 5.7 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 81.0/100.2 MB 5.7 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 81.5/100.2 MB 5.7 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 82.3/100.2 MB 5.6 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 82.8/100.2 MB 5.6 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 83.6/100.2 MB 5.5 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 84.4/100.2 MB 5.5 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 85.2/100.2 MB 5.5 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 86.0/100.2 MB 5.5 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 86.8/100.2 MB 5.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 87.8/100.2 MB 5.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 88.6/100.2 MB 5.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 89.7/100.2 MB 5.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 90.4/100.2 MB 5.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 91.2/100.2 MB 5.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 91.8/100.2 MB 5.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 92.3/100.2 MB 5.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 92.8/100.2 MB 5.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 93.3/100.2 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 94.4/100.2 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 95.2/100.2 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.9/100.2 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 96.7/100.2 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  97.8/100.2 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.6/100.2 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.6/100.2 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  100.1/100.2 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 100.2/100.2 MB 5.1 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: np\n",
      "  Building wheel for np (pyproject.toml): started\n",
      "  Building wheel for np (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for np: filename=np-1.0.2-py3-none-any.whl size=8034 sha256=fb71068af4d77ead823217e95c98ea1d1322442fe8950576cdbc8429fa2967e6\n",
      "  Stored in directory: c:\\users\\msi cyborg 14\\appdata\\local\\pip\\cache\\wheels\\79\\e4\\5a\\ab99765c3abba370362c941a262f2b1cb97fe9d8a766f6bea6\n",
      "Successfully built np\n",
      "Installing collected packages: np, einops, xformers\n",
      "\n",
      "   ------------- -------------------------- 1/3 [einops]\n",
      "   ------------- -------------------------- 1/3 [einops]\n",
      "   ------------- -------------------------- 1/3 [einops]\n",
      "   ------------- -------------------------- 1/3 [einops]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   -------------------------- ------------- 2/3 [xformers]\n",
      "   ---------------------------------------- 3/3 [xformers]\n",
      "\n",
      "Successfully installed einops-0.8.1 np-1.0.2 xformers-0.0.32.post2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import ModuleList\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from torch import nn\n",
    "\n",
    "import copy\n",
    "import math"
   ],
   "metadata": {
    "id": "wJjFQShadGTx",
    "ExecuteTime": {
     "end_time": "2025-09-17T05:50:55.241446Z",
     "start_time": "2025-09-17T05:50:54.960651Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "`np.float_` was removed in the NumPy 2.0 release. Use `np.float64` instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mnn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnn\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\profile\\programing\\ia\\BasicLLMsModels\\.venv\\Lib\\site-packages\\np\\__init__.py:8\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# -*- coding: utf-8 -*-\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m# Author: Koos Zevenhoven\u001B[39;00m\n\u001B[32m      3\u001B[39m \n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# Needs to do some magic to import numpy and replace it with a monkey-patched\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# version implementing __getitem__ etc. This seems to be close to the cleanest\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[38;5;66;03m# way to do this in Python. This now works best with Python >=3.5\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmagic\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\profile\\programing\\ia\\BasicLLMsModels\\.venv\\Lib\\site-packages\\np\\magic.py:7\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# -*- coding: utf-8 -*-\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[33;03mCreated on Mon Mar 28 00:54:30 2016\u001B[39;00m\n\u001B[32m      4\u001B[39m \n\u001B[32m      5\u001B[39m \u001B[33;03m@author: Koos Zevenhoven\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mquickarrays\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mquickarrays\u001B[39;00m\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msys\u001B[39;00m\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m sys.version_info < (\u001B[32m3\u001B[39m,\u001B[32m5\u001B[39m,\u001B[32m0\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\profile\\programing\\ia\\BasicLLMsModels\\.venv\\Lib\\site-packages\\np\\quickarrays.py:19\u001B[39m\n\u001B[32m     13\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mThe numpy package must be installed to use the np extensions. \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m     14\u001B[39m                        \u001B[33m\"\u001B[39m\u001B[33mPlease install numpy using your package manager such as conda or pip.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m array, asarray, asanyarray, isscalar, shape\n\u001B[32m     18\u001B[39m np_quick_types = {\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m: \u001B[43mnumpy\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfloat_\u001B[49m,\n\u001B[32m     20\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mf8\u001B[39m\u001B[33m'\u001B[39m: numpy.float64,\n\u001B[32m     21\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mf64\u001B[39m\u001B[33m'\u001B[39m: numpy.float64,\n\u001B[32m     22\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mf4\u001B[39m\u001B[33m'\u001B[39m: numpy.float32,\n\u001B[32m     23\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mf32\u001B[39m\u001B[33m'\u001B[39m: numpy.float32,\n\u001B[32m     24\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mf2\u001B[39m\u001B[33m'\u001B[39m: numpy.float16,\n\u001B[32m     25\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mi\u001B[39m\u001B[33m'\u001B[39m: numpy.int_,\n\u001B[32m     26\u001B[39m \u001B[38;5;66;03m#    'i8': numpy.int64,  allow this in the future?\u001B[39;00m\n\u001B[32m     27\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mi64\u001B[39m\u001B[33m'\u001B[39m: numpy.int64,\n\u001B[32m     28\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mi4\u001B[39m\u001B[33m'\u001B[39m: numpy.int32,\n\u001B[32m     29\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mi32\u001B[39m\u001B[33m'\u001B[39m: numpy.int32,\n\u001B[32m     30\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mi2\u001B[39m\u001B[33m'\u001B[39m: numpy.int16,\n\u001B[32m     31\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mi16\u001B[39m\u001B[33m'\u001B[39m: numpy.int16,\n\u001B[32m     32\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mi1\u001B[39m\u001B[33m'\u001B[39m: numpy.int8,\n\u001B[32m     33\u001B[39m \u001B[38;5;66;03m#    'i8': numpy.int8,   not ok; this might mean 8*8 == 64 bits\u001B[39;00m\n\u001B[32m     34\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mu8\u001B[39m\u001B[33m'\u001B[39m: numpy.uint64,\n\u001B[32m     35\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mui64\u001B[39m\u001B[33m'\u001B[39m: numpy.uint64,\n\u001B[32m     36\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mu4\u001B[39m\u001B[33m'\u001B[39m: numpy.uint32,\n\u001B[32m     37\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mui32\u001B[39m\u001B[33m'\u001B[39m: numpy.uint32,\n\u001B[32m     38\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mu2\u001B[39m\u001B[33m'\u001B[39m: numpy.uint16,\n\u001B[32m     39\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mui16\u001B[39m\u001B[33m'\u001B[39m: numpy.uint16,\n\u001B[32m     40\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mu1\u001B[39m\u001B[33m'\u001B[39m: numpy.uint8,\n\u001B[32m     41\u001B[39m \u001B[38;5;66;03m#    'ui8': numpy.uint8, not ok; this might mean 8*8 == 64 bits \u001B[39;00m\n\u001B[32m     42\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mc\u001B[39m\u001B[33m'\u001B[39m: numpy.complex_,\n\u001B[32m     43\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mc16\u001B[39m\u001B[33m'\u001B[39m: numpy.complex128,\n\u001B[32m     44\u001B[39m \u001B[38;5;66;03m#    'c128': numpy.complex128,  leave out for consistency\u001B[39;00m\n\u001B[32m     45\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mc8\u001B[39m\u001B[33m'\u001B[39m: numpy.complex64,\n\u001B[32m     46\u001B[39m \u001B[38;5;66;03m#    'c64': numpy.complex64     leave out for consistency\u001B[39;00m\n\u001B[32m     47\u001B[39m }\n\u001B[32m     49\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mall_scalar\u001B[39m(objs):\n\u001B[32m     50\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mall\u001B[39m(isscalar(obj) \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m objs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\profile\\programing\\ia\\BasicLLMsModels\\.venv\\Lib\\site-packages\\numpy\\__init__.py:794\u001B[39m, in \u001B[36m__getattr__\u001B[39m\u001B[34m(attr)\u001B[39m\n\u001B[32m    791\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(__former_attrs__[attr], name=\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    793\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m attr \u001B[38;5;129;01min\u001B[39;00m __expired_attributes__:\n\u001B[32m--> \u001B[39m\u001B[32m794\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[32m    795\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m`np.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattr\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m` was removed in the NumPy 2.0 release. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    796\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m__expired_attributes__[attr]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    797\u001B[39m         name=\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    798\u001B[39m     )\n\u001B[32m    800\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m attr == \u001B[33m\"\u001B[39m\u001B[33mchararray\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    801\u001B[39m     warnings.warn(\n\u001B[32m    802\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m`np.chararray` is deprecated and will be removed from \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    803\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mthe main namespace in the future. Use an array with a string \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    804\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mor bytes dtype instead.\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m, stacklevel=\u001B[32m2\u001B[39m)\n",
      "\u001B[31mAttributeError\u001B[39m: `np.float_` was removed in the NumPy 2.0 release. Use `np.float64` instead."
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "id": "gu3RiwkfeXJU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfXyaTFSoVI7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738365035781,
     "user_tz": 300,
     "elapsed": 3,
     "user": {
      "displayName": "Jhenner Tigreros",
      "userId": "10282691994518148206"
     }
    },
    "outputId": "fc565b2b-181d-48d9-e501-0df3eba6a7c1"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def _get_clones(module, n):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(n)])"
   ],
   "metadata": {
    "id": "FlUvCj4dfE1y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Conv1D(nn.Module):\n",
    "    def __init__(self, nx, nf):\n",
    "        '''\n",
    "        nx: Numero de datos de entrada.\n",
    "        nf: Numero de filtros. (Canales de salida).\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.nf = nf\n",
    "        #Inicializando una matriz vacia de pesos del tamaño (nx)X(nf)\n",
    "        w = torch.empty(nx, nf)\n",
    "        #Inicializando los pesos con una distribución normal.\n",
    "        nn.init.normal_(w, std=0.02)\n",
    "        #Calculando los pesos y sesgos encodeandos usando nn.Parameter\n",
    "        self.weight = nn.Parameter(w)\n",
    "        self.bias = nn.Parameter(torch.zeros(nf))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''x:Tensor de entrada.'''\n",
    "        #El tamaño de la salida es la suna de la segunda dimensión de X y el número de filtros nf.\n",
    "        size_out = x.size()[:-1] + (self.nf,)\n",
    "        # Producot punto Q,K(Transpuesta) y V\n",
    "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)# x.view ayuda a calcular la transpuesta.\n",
    "        x = x.view(*size_out)\n",
    "        return x"
   ],
   "metadata": {
    "id": "Kfp70gl_fHoL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dropout, d_model=768, nx=768*4):\n",
    "        super().__init__()\n",
    "        self.c_fc    = Conv1D(d_model, nx)\n",
    "        self.c_proj  = Conv1D(nx, d_model)\n",
    "        self.act     = F.gelu\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.act(self.c_fc(x))))"
   ],
   "metadata": {
    "id": "Xms1IIJUfO8j"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=768, n_head=12, n_ctx=1024, d_head=64, bias=True, scale=False):\n",
    "        '''Función de construcción\n",
    "        Params:\n",
    "        d_model:Dimensión que necesita ser ingresada en el modelo.\n",
    "        n_head:La cantidad de heads de atención.\n",
    "        n_ctx:Buffer para guardar los registros del sesgo.\n",
    "        d_head:Dimesión de salida para el head.\n",
    "        bias:Un booleano para saber si incluir el sesgo.\n",
    "        scale: Escalar y estabilidad númerica (sqrt(dk))\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.n_head  = n_head\n",
    "        self.d_model = d_model\n",
    "        self.c_attn  = Conv1D(d_model, d_model*3)\n",
    "        self.scale   = scale\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.c_proj  = Conv1D(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Diviendo en la cantidad de heads y retornando.\n",
    "        return shape [`batch`, `head`, `sequence`, `features`]\n",
    "        \"\"\"\n",
    "        new_shape = x.size()[:-1] + (self.n_head, x.size(-1)//self.n_head)\n",
    "        x = x.view(*new_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def _attn(self, q, k, v, attn_mask=None):\n",
    "        \"\"\"Función de antención principal.\n",
    "        Que calcula usando la formula de producto punto de atención.\"\"\"\n",
    "        scores  = torch.matmul(q, k.transpose(-2, -1))# producto punto de Q*K(t)\n",
    "        if self.scale: scores = scores/math.sqrt(v.size(-1))# escalandola por sqrt(dk)\n",
    "        nd, ns  = scores.size(-2), scores.size(-1)\n",
    "        if attn_mask is not None: scores = scores + attn_mask# agregando los valores con la mascara de atención.\n",
    "        scores  = self.softmax(scores)# añadiendo los valores de softmax\n",
    "        scores  = self.dropout(scores) # función de dropout 0.1\n",
    "        outputs = torch.matmul(scores, v) # Multiplicación final del puntaje por V.\n",
    "        return outputs\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        # Combinando todas las heads en una sola.\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        new_shape = x.size()[:-2] + (x.size(-2)*x.size(-1),)\n",
    "        return x.view(*new_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Función de para calcular atención, separar las heads y combinarlas de nuevo.'''\n",
    "        x        = self.c_attn(x) #new `x` shape - `[1,3,2304]`\n",
    "        q, k, v  = x.split(self.d_model, dim=2)\n",
    "        q, k, v  = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
    "        out      = self._attn(q, k, v)\n",
    "        out      = self.merge_heads(out)\n",
    "        out      = self.c_proj(out)\n",
    "        return out"
   ],
   "metadata": {
    "id": "uravxQVpfZ4S"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model=768, n_head=12, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn        = Attention(d_model=768, n_head=12, d_head=64, n_ctx=1024, bias=True, scale=False)\n",
    "        self.feedforward = FeedForward(dropout=0.1, d_model=768, nx=768*4)\n",
    "        self.ln_1        = LayerNorm(d_model)\n",
    "        self.ln_2        = LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.feedforward(self.ln_2(x))\n",
    "        return x"
   ],
   "metadata": {
    "id": "6tSwyE6QfiLP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self, nlayers=12, n_ctx=1024, d_model=768, vcb_sz=50257):\n",
    "        '''nlayer: La cantidad de veces que queremos multiplicar el Transformer.\n",
    "        n_ctx: El contexto, la cantidad total de tokens que puede ver en el pasado de las palabras.\n",
    "        d_model:Dimesionos del modelo.\n",
    "        vcb_sz:El tamaño del vocabulario usado en el entrenamiento.'''\n",
    "        super(GPT2, self).__init__()\n",
    "        self.nlayers = nlayers\n",
    "        block        = TransformerBlock(d_model=768, n_head=12, dropout=0.1)\n",
    "        self.h       = _get_clones(block, 12)\n",
    "        self.wte     = nn.Embedding(vcb_sz, d_model)\n",
    "        self.wpe     = nn.Embedding(n_ctx, d_model)\n",
    "        self.drop    = nn.Dropout(0.1)\n",
    "        self.ln_f    = LayerNorm(d_model)\n",
    "        self.out     = nn.Linear(d_model, vcb_sz, bias=False)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        '''Inicialización de los pesos.'''\n",
    "        self.out.weight = self.wte.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        '''Inicialización con la media y S.D.'''\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n",
    "                '''Data Bias zero'''\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, src, labels=None, pos_ids=None):\n",
    "        '''Añadir el embedding posicional, dropping y añadiendo los inputs\n",
    "           usados por la función de perdida y finalmente añadiendo la salida y la\n",
    "           perdida.'''\n",
    "        if pos_ids is None:\n",
    "            pos_ids = torch.arange(0, src.size(-1)).unsqueeze(0)\n",
    "        pos_ids = pos_ids.to(src.device)  # Asegurarse que los pos_ids están en el mismo device.\n",
    "        inp = self.drop((self.wte(src) + self.wpe(pos_ids)))\n",
    "        for i in range(self.nlayers): inp = self.h[i](inp)\n",
    "        inp     = self.ln_f(inp)\n",
    "        logits  = self.out(inp)\n",
    "        outputs = (logits,) + (inp,)\n",
    "\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "            return loss.mean()\n",
    "        return logits"
   ],
   "metadata": {
    "id": "Cm2dD9KyflYi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "import time\n",
    "from transformers import GPT2Tokenizer"
   ],
   "metadata": {
    "id": "Q1mPKZxdftI6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = GPT2()"
   ],
   "metadata": {
    "id": "ZuoC4itMfxqz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!curl --output gpt2-pytorch_model.bin https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmKHsWiHf7e9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738362900585,
     "user_tz": 300,
     "elapsed": 69534,
     "user": {
      "displayName": "Jhenner Tigreros",
      "userId": "10282691994518148206"
     }
    },
    "outputId": "a8d915ff-bdd1-47e9-8cb3-0ef8d4787e99"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  522M  100  522M    0     0  7761k      0  0:01:08  0:01:08 --:--:-- 13.4M\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model_dict = model.state_dict()\n",
    "state_dict = torch.load(\"./gpt2-pytorch_model.bin\")\n",
    "\n",
    "old_keys = []\n",
    "new_keys = []\n",
    "for key in state_dict.keys():\n",
    "    if \"mlp\" in key: #El diccionario de estado para el MLP feedforward debe ser cambiado por mlp\n",
    "        new_key = key.replace(\"mlp\", \"feedforward\")\n",
    "        new_keys.append(new_key)\n",
    "        old_keys.append(key)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bqw4UaZKgLrm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738366651100,
     "user_tz": 300,
     "elapsed": 779,
     "user": {
      "displayName": "Jhenner Tigreros",
      "userId": "10282691994518148206"
     }
    },
    "outputId": "03e37066-ef69-4693-b3b1-f8171a1a6853"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-51-0d65074896ff>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"./gpt2-pytorch_model.bin\")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for old_key, new_key in zip(old_keys, new_keys):\n",
    "    state_dict[new_key]=state_dict.pop(old_key)"
   ],
   "metadata": {
    "id": "3r3EXVWNgV5A"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n",
    "\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "model.eval()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Z_H4pVIgX3D",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738366668375,
     "user_tz": 300,
     "elapsed": 371,
     "user": {
      "displayName": "Jhenner Tigreros",
      "userId": "10282691994518148206"
     }
    },
    "outputId": "7ddc19a2-4b43-4c90-c405-7dd7e2e6a4ff"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GPT2(\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())"
   ],
   "metadata": {
    "id": "YllyrazTgeG8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "size_bytes = total_params * 4\n",
    "size_mb = size_bytes / (1024 ** 2)\n",
    "\n",
    "print(f\"El tamaño total de GPT2 sin alteraciones es: {size_bytes} bytes o {size_mb:.2f} MB\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DZCeOM8gf8h",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738366723027,
     "user_tz": 300,
     "elapsed": 372,
     "user": {
      "displayName": "Jhenner Tigreros",
      "userId": "10282691994518148206"
     }
    },
    "outputId": "8a755b70-4704-41e7-892f-dc1dc00124c7"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "El tamaño total de GPT2 sin alteraciones es: 497759232 bytes o 474.70 MB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "context = torch.tensor([tokenizer.encode(\"The planet earth is a beautiful\")])"
   ],
   "metadata": {
    "id": "voxmoWW3go6l"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def generate(context, ntok=550):\n",
    "    start_time = time.time()\n",
    "    for _ in range(ntok):\n",
    "        out = model(context)\n",
    "        logits = out[:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = -np.inf\n",
    "        next_tok = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    return context, inference_time"
   ],
   "metadata": {
    "id": "StTNnLCbg1xM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "out, inference_time = generate(context, ntok=40)\n",
    "decoded_output = tokenizer.decode(out[0])"
   ],
   "metadata": {
    "id": "1JzzXApkg_oP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
    "print(f\"Generated Output: {decoded_output}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_LUY2n9IiBS8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738366816323,
     "user_tz": 300,
     "elapsed": 368,
     "user": {
      "displayName": "Jhenner Tigreros",
      "userId": "10282691994518148206"
     }
    },
    "outputId": "b508de58-3ae8-49ee-b0b8-7f9e74bd220b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Inference Time: 7.6916 seconds\n",
      "Generated Output: The planet earth is a beautiful place that.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "( ) a: a) \" is a planet a \" a. a planet, a a. a. place the\n",
      "\n",
      ": a:\n"
     ]
    }
   ]
  }
 ]
}
